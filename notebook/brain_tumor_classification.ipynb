{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e233234",
   "metadata": {},
   "source": [
    "# Brainn Tumor Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553fa7a1",
   "metadata": {},
   "source": [
    "# Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "439eb45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\python-projects\\\\brain-tumor-classification'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "635ffc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from src.utils.common import read_yaml, create_directories\n",
    "from src.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from src.logging import logger\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08c212b",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8fd73aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_data_dir: Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7970c3",
   "metadata": {},
   "source": [
    "The *DataIngestionConfig Class* store two important paths for data processing, and the \"frozen\" part ensures these paths can't be accidentally modified later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c802ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([Path(self.config.artifacts_root)])\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "        create_directories([Path(config.root_dir)])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=Path(config.root_dir),  \n",
    "            source_data_dir=Path(config.source_data_dir)\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f35e54b",
   "metadata": {},
   "source": [
    "*ConfigurationManager*  is a class that helps to manage and organize your project's settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d02073b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def initiate_data_ingestion(self):\n",
    "        logger.info(\"Checking Data Directory is exsisting\")\n",
    "        if not self.config.source_data_dir.exists():\n",
    "            raise FileNotFoundError(f\"Data directory not found at {self.config.source_data_dir}\")\n",
    "        logger.info(\"Data directory found and ready to use\")\n",
    "        return self.config.source_data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d9ac1",
   "metadata": {},
   "source": [
    "*DataIngestion* class is responsible for handling the data ingestion process, meaning it helps to check if the data directory exists and can be used for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a19bcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-05 15:49:23,304] [14] [tumorClassifierLogger] - INFO - YAML file loaded successfully: config\\config.yaml\n",
      "[2025-05-05 15:49:23,307] [14] [tumorClassifierLogger] - INFO - YAML file loaded successfully: params.yaml\n",
      "[2025-05-05 15:49:23,308] [27] [tumorClassifierLogger] - INFO - Created directory at: artifacts\n",
      "[2025-05-05 15:49:23,309] [27] [tumorClassifierLogger] - INFO - Created directory at: artifacts\\data_ingestion\n",
      "[2025-05-05 15:49:23,310] [6] [tumorClassifierLogger] - INFO - Checking Data Directory is exsisting\n",
      "[2025-05-05 15:49:23,311] [9] [tumorClassifierLogger] - INFO - Data directory found and ready to use\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "    data_ingestion.initiate_data_ingestion()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23026db6",
   "metadata": {},
   "source": [
    "# Prepare Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf5c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class PrepareBaseModelConfig:\n",
    "    root_dir: Path\n",
    "    base_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    params_image_size: list\n",
    "    params_learning_rate: float\n",
    "    params_include_top: bool\n",
    "    params_weights: str\n",
    "    params_classes: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25669a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([Path(self.config.artifacts_root)])\n",
    "\n",
    "\n",
    "    def get_prepare_base_model_config(self) -> PrepareBaseModelConfig:\n",
    "        config = self.config.prepare_base_model\n",
    "        \n",
    "        create_directories([Path(config.root_dir)])\n",
    "        \n",
    "\n",
    "        prepare_base_model_config = PrepareBaseModelConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            base_model_path=Path(config.base_model_path),\n",
    "            updated_base_model_path=Path(config.updated_base_model_path),\n",
    "            params_image_size=self.params.IMAGE_SIZE,\n",
    "            params_learning_rate=self.params.LEARNING_RATE,\n",
    "            params_include_top=self.params.INCLUDE_TOP,\n",
    "            params_weights=self.params.WEIGHTS,\n",
    "            params_classes=self.params.CLASSES\n",
    "        )\n",
    "\n",
    "        return prepare_base_model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f63face6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PrepareBaseModel:\n",
    "    def __init__(self, config: PrepareBaseModelConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def get_base_model(self):\n",
    "        self.model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        if not self.config.params_include_top:\n",
    "            self.model.classifier = nn.Identity()\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.save_model(path=self.config.base_model_path, model=self.model)\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_full_model(model, classes, freeze_all, freeze_till, learning_rate):\n",
    "        if freeze_all:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        elif (freeze_till is not None) and (freeze_till > 0):\n",
    "            for idx, child in enumerate(model.features):\n",
    "                if idx < freeze_till:\n",
    "                    for param in child.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(25088, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        model.to(model.device if hasattr(model, 'device') else torch.device(\"cpu\"))\n",
    "        return model\n",
    "\n",
    "    def update_base_model(self):\n",
    "        self.full_model = self._prepare_full_model(\n",
    "            model=self.model,\n",
    "            classes=self.config.params_classes,\n",
    "            freeze_all=True,\n",
    "            freeze_till=None,\n",
    "            learning_rate=self.config.params_learning_rate\n",
    "        )\n",
    "        self.save_model(self.config.updated_base_model_path, self.full_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(path, model):\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "\n",
    "    \n",
    "    def train_model(self, train_loader: DataLoader, val_loader: DataLoader = None, epochs: int = 5):\n",
    "        model = self.full_model.to(self.device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=self.config.params_learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{epochs}]\", leave=False)\n",
    "            for inputs, labels in loop:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loop.set_postfix(loss=running_loss / (total if total else 1),\n",
    "                                 acc=100. * correct / total if total else 0)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss:.4f}, Accuracy: {100. * correct / total:.2f}%\")\n",
    "\n",
    "            # Optional: validation\n",
    "            if val_loader:\n",
    "                self.evaluate_model(model, val_loader)\n",
    "\n",
    "        self.save_model(self.config.updated_base_model_path, model)\n",
    "\n",
    "    def evaluate_model(self, model, val_loader):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print(f\"Validation Accuracy: {100. * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da06d65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-17 23:52:00,297] [14] [tumorClassifierLogger] - INFO - YAML file loaded successfully: config\\config.yaml\n",
      "[2025-05-17 23:52:00,300] [14] [tumorClassifierLogger] - INFO - YAML file loaded successfully: params.yaml\n",
      "[2025-05-17 23:52:00,302] [27] [tumorClassifierLogger] - INFO - Created directory at: artifacts\n",
      "[2025-05-17 23:52:00,303] [27] [tumorClassifierLogger] - INFO - Created directory at: artifacts\\prepare_base_model\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\Amir/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528M/528M [17:33<00:00, 525kB/s] \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    prepare_base_model_config = config.get_prepare_base_model_config()\n",
    "    prepare_base_model = PrepareBaseModel(config=prepare_base_model_config)\n",
    "    prepare_base_model.get_base_model()\n",
    "    prepare_base_model.update_base_model()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a4cdff",
   "metadata": {},
   "source": [
    "# Prepare Callbaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "677f8467",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class PrepareCallbacksConfig:\n",
    "    root_dir: Path\n",
    "    tensorboard_root_log_dir: Path\n",
    "    checkpoint_model_filepath: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c560457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([Path(self.config.artifacts_root)])\n",
    "\n",
    "    \n",
    "    def get_prepare_callback_config(self) -> PrepareCallbacksConfig:\n",
    "        config = self.config.prepare_callbacks\n",
    "        model_ckpt_dir = os.path.dirname(config.checkpoint_model_filepath)\n",
    "        create_directories([\n",
    "            Path(model_ckpt_dir),\n",
    "            Path(config.tensorboard_root_log_dir)\n",
    "        ])\n",
    "\n",
    "        prepare_callback_config = PrepareCallbacksConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            tensorboard_root_log_dir=Path(config.tensorboard_root_log_dir),\n",
    "            checkpoint_model_filepath=Path(config.checkpoint_model_filepath)\n",
    "        )\n",
    "\n",
    "        return prepare_callback_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bbbb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareCallback:\n",
    "    def __init__(self, config:PrepareCallbacksConfig):\n",
    "        self.config = config\n",
    "        self.timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        self.tb_running_log_dir = os.path.join(\n",
    "            self.config.tensorboard_root_log_dir,\n",
    "            f\"tb_logs_at_{self.timestamp}\",\n",
    "        )\n",
    "        self._writer = None\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    @property\n",
    "    def _create_tb_callbacks(self):\n",
    "        \"\"\"Equivalent of TensorBoard callback in TensorFlow\"\"\"\n",
    "        if self._writer is None:\n",
    "            self._writer = SummaryWriter(log_dir=self.tb_running_log_dir)\n",
    "        return self._writer\n",
    "\n",
    "    @property\n",
    "    def _create_ckpt_callbacks(self):\n",
    "        \"\"\"Equivalent of ModelCheckpoint in TensorFlow\"\"\"\n",
    "        def checkpoint_callback(model, current_loss):\n",
    "            if current_loss < self.best_loss:\n",
    "                self.best_loss = current_loss\n",
    "                torch.save(model.state_dict(), self.config.checkpoint_model_filepath)\n",
    "                print(f\"[Checkpoint] Saved model with loss {current_loss:.4f}\")\n",
    "        return checkpoint_callback\n",
    "\n",
    "    def get_tb_ckpt_callbacks(self):\n",
    "        return [\n",
    "            self._create_tb_callbacks,\n",
    "            self._create_ckpt_callbacks\n",
    "        ]\n",
    "\n",
    "    def close(self):\n",
    "        if self._writer is not None:\n",
    "            self._writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7cb1327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-18 13:08:52,151] [14] [tumorClassifierLogger] - INFO - YAML file loaded successfully: config\\config.yaml\n",
      "[2025-05-18 13:08:52,155] [14] [tumorClassifierLogger] - INFO - YAML file loaded successfully: params.yaml\n",
      "[2025-05-18 13:08:52,157] [27] [tumorClassifierLogger] - INFO - Created directory at: artifacts\n",
      "[2025-05-18 13:08:52,158] [27] [tumorClassifierLogger] - INFO - Created directory at: artifacts\\prepare_callbacks\\checkpoint_dir\n",
      "[2025-05-18 13:08:52,159] [27] [tumorClassifierLogger] - INFO - Created directory at: artifacts\\prepare_callbacks\\tensorboard_log_dir\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    prepare_callbacks_config = config.get_prepare_callback_config()\n",
    "    prepare_callbacks = PrepareCallback(config=prepare_callbacks_config)\n",
    "    callback_list = prepare_callbacks.get_tb_ckpt_callbacks()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
